---
title: "Reproducible Research in Practice"
# format: revealjs
author:
  - name: Neil Shephard
    orcid: 0000-0001-8301-6857
    email: n.shephard@sheffield.ac.uk
    affiliations: Employers Name
from: markdown+emoji
format:
  clean-revealjs:
    incremental: false
    slide-number: true
    show-slide-number: speaker
    auto-stretch: false
    chalkboard: true
    # embed-resources: true
    # standalone: true
    header: Reproducible Research in Practice
revealjs-plugins:
  - confetti
footer: "**Slides** : [**ns-rse.github.io/reproducible_research_example**](https://ns-rse.github.io/reproducible_research_example)"
project:
  preview:
    port: 7864
    host: localhost
    watch-inputs: true
filters:
  - openlinksinnewpage
  - reveal-header
---

## Scan This

{{< qrcode <https://ns-rse.github.io/reproducible_research_example> qr1 width=400 height=400 >}}

[ns-rse.github.io/reproducible_research_example](https://ns-rse.github.io/reproducible_research_example)

## Who Am I/What Do I Do?

:::: {.columns}

::: {.column width="50%"}

+ Background in evolutionary and statistical genetics.
+ Genetic Statistician
+ Medical Statistician
+ Data Scientist
+ :tada: [Research Software Engineer](https://rse.shef.ac.uk) :tada:

:::

::: {.column width="50%"}

+ Linux, R, Python, Bash
+ Literate Programming
+ Reproducible Research
+ Open Research
+ Free Open Source Software

:::
::::

::: {.notes}
Before we get into the details I thought it might be useful to explain who am I am and why I'm here to talk to you
today.

I started my academic career studying Zoology and Genetics at Undergraduate then moved onto a Masters in Genetic
Epidemiology, both here at the University of Sheffield quite a few years ago.  After graduating I spent about eight or
nine years as a Genetic Statistician trying to work out what genes are involved in complex diseases at various
Universities, Manchester, Western Australia and then returning to Sheffield. I then shifted careers to work as a Medical
Statistician for about nine years followed by four years in industry as a Data Scientist where I didn't actually do much
data science but did learn a lot about software development which is how I've ended up back at the University of
Sheffield as a Research Software Engineer.

A common theme from my Masters on-wards is that I have used computers to write programmes and analyse data. I've taught
myself Linux system administration and from a rudimentary training on Fortran, C and Stata have gone on to learn Bash, R
and Python as well as a few other bits and pieces of different languages.

Very early on in my career I developed an interest in literate programming and reproducible research which is why Aneta
has invited me her today to talk to you and I'll expand on these topics and show you some work I'm currently engaged in
and how I undertake it in a reproducible manner.

Reproducible Research is one of the foundational aspects of Open Research as by ensuring our work is reproducible and
open to review it improves confidence in its accuracy and reliability.
:::


## Reproducible Research

+ Reproducibility is fundamental to the scientific method.
+ Traditionally hand-written notebooks with methods shared in publications.
+ Modern equivalent is code...
  + Software
  + Statistical scripts
    + Data cleaning
    + Analysis, graphs & tables
    + Numbers and statistics


## Literate Programming - Origins

:::: {.columns}
::: {.column width="50%"}
> a computer program is given as an explanation of how it works in a natural language, such as English, interspersed
(embedded) with snippets of macros and traditional source code, from which compilable source code can be
generated.^[[Wikipedia - Literate programming](https://en.wikipedia.org/wiki/Literate_programming)]
:::
::: {.column width="50%"}
![[_Literate Programming_ - Donald Knuth](https://en.wikipedia.org/wiki/Literate_programming)](https://upload.wikimedia.org/wikipedia/en/6/62/Literate_Programming_book_cover.jpg)
:::
::::

::: {.notes}
RMarkdown and other languages like it such as the newer [Quarto][quarto] and Emacs'
[Org-mode](https://orgmode.org) are literate programming languages which may be an unfamiliar term so I've borrowed the
definition from Wikipedia.

The idea was originally proposed and developed the computer scientist Donald Knuth, and he literally wrote the book on it.
:::

## Literate Programming - Better Programmes

:::: {.columns}
::: {.column width="50%"}
> My programs are not only explained better than ever before; they also are better programs, because the new methodology
encourages me to do a better job.^[[Knuth (1984)](http://www.literateprogramming.com/knuthweb.pdf)]
:::
::: {.column width="50%"}
![[_Literate Programming_ - Donald Knuth](https://en.wikipedia.org/wiki/Literate_programming)](https://upload.wikimedia.org/wikipedia/en/6/62/Literate_Programming_book_cover.jpg)
:::
::::

::: {.notes}
Whilst originating in computer science the principle has seen widespread adoption in research and data science because
it encourages reproducible research and facilitates open access to research.

Also, as Knuth found when he adopted the practice it _improved_ the quality of the programmes he wrote, and I've found
the same is true when writing documents myself since adopting the practice over 20 years ago.
:::

## What's Involved?

+ Logical, ordered directory structure.
+ Scripts/code.
+ Version control of code.
+ Reproducible environment (R packages using [renv](https://rstudio.github.io/renv/articles/renv.html)).

::: {.notes}
It can seem overwhelming learning a new paradigm or framework for undertaking work and I've often found that people want
to get on and do the work rather than take a step back and assess how they are going about it but its well worth taking
the time out to review and assess your working practices and see how they can be improved.

I'm going to walk you through a project that I've been working on that is looking to improve the prediction of Thyroid
Cancer. Its not my own work, its the topic for a PhD for a clinicial Dr Ovie Edafe who is a specialist in neck cancers
and related surgery.

There are four broad areas that I'll cover that hopefully align with the content you have already covered and will be
covering in the coming weeks.

The first step is having an organised structure to your directories having data organised in a consistent and structured
manner makes it considerably easier to work with.

You then need to write code or scripts, which are still code, that defines the steps you undertake in cleaning and
tidying your data and getting it ready for analysis before another series of scripts are used to run the analysis.

All of the code that is written should be version controlled which allos you to track the history of the code and see
what changes have been made over time. If a mistake is made it is possible to revert back to a previous state where
things did work. Version control and tools such as Git and GitHub make it much easier to collaborate with others on
research too.

One final step is having a reproducible environment under which your work is undertaken. This means the specific
versions of packages that you have used for analysis should be recorded and used again in the future. In R this can be
done via the [`renv`]() package. Its not without problems but the situation today is far better than it used to be.
:::

## Directory Structure

+ Important to keep a copy of the raw data as received.
+ Data _always_ needs cleaning.
+ ~80% of data analysis is data cleaning/wrangling data!
+ _Always_ keep a record of changes you make to the data.
+ Separate directories for different data types and script/markdown files.

::: {.notes}
When undertaking research its imperative to keep a copy of the raw data as received. This might be data from sensors
that have captured data and if you work with this sort of data then its likely to be fairly well structured, but can
also be the results of surveys that have been captured and stored in databases and then exported for you to work
with. Depending on how well the data capture was setup you will almost invariably have to clean your data, in my
experience about 80% of data analysis is cleaning the data and wrangling it such as deriving variables like age from
date of birth, and on that note if you ever do want to analyse age as a variable try and capture date of birth as you
can easily derive the age in years or categorise it into bins, but if you only ask people to say what age band they are
in you can't go back the other way. Common errors in data cleaning are having string characters in variables that are
meant to be numeric, or having categorical variables where people haven't entered the correct field and so you have to
then work out how to tidy this which I'll come on to.

The key thin is that its important to keep a record of the changes you make and these will be in R scripts or
Markdown/Quarto files. And it therefore makes sense to separate your data from your scripts.
:::

## Directory Structure - Thyroid Cancer Prediction

:::: {.columns}

::: {.column width="60%"}

```
❱ tree -d -L 2
[4.0K Mar 19 12:36]  .
├── [4.0K Nov 22 14:57]  ./data
│   ├── [4.0K Jan  9 13:32]  ./data/csv
│   └── [4.0K Feb 29 17:27]  ./data/r
├── [4.0K Mar 14 17:36]  ./docs
│   ├── [   7 Jan  5 14:45]  ./docs/data -> ../data
│   ├── [4.0K Feb 29 15:38]  ./docs/modelling_cache
│   ├── [4.0K Mar 14 17:21]  ./docs/modelling_files
│   ├── [   4 Jan  5 14:44]  ./docs/r -> ../r
│   └── [4.0K Mar 14 17:21]  ./docs/_site
├── [4.0K Oct 19 11:40]  ./inst
├── [4.0K Nov 16 10:54]  ./log
├── [4.0K Oct 19 11:24]  ./node_modules
├── [4.0K Oct 19 12:39]  ./pages
├── [4.0K Oct 12 10:43]  ./papers
├── [4.0K Nov  9 11:44]  ./quarto
│   └── [4.0K Oct 19 13:12]  ./quarto/_site
├── [4.0K Mar 14 16:59]  ./r
├── [4.0K Mar 14 17:28]  ./renv
│   ├── [4.0K Nov 16 10:57]  ./renv/library
│   └── [4.0K Feb 29 14:28]  ./renv/staging
├── [4.0K Jan 31 11:00]  ./_site
│   ├── [4.0K Dec 19 16:36]  ./_site/data_files
│   ├── [4.0K Feb 15 15:15]  ./_site/docs
│   ├── [4.0K Dec 19 16:36]  ./_site/modelling_files
│   └── [4.0K Jan 25 17:22]  ./_site/site_libs
└── [4.0K Feb  1 17:47]  ./tmp

29 directories
```

:::
::: {.column width="40%"}

Key top-level directories

+ `data/` - holds data.
+ `docs/` - _my_ Quarto files.
+ `r/` - _my_ R-scripts.
:::
::::


::: {.notes}
Here is an example of the directory structure of one of the repositories I have setup for this work.

The output only shows the nesting two directory levels deep, but you can see that at the top level I have a number of
directories, not all of them are relevant, but the key ones are listed on the right, there is the `data/` directory
which will hold all of the data, there are two sub-directories `csv` for raw CSV files and `r` which holds cleaned data
in R's byte format which speeds up loading and preserves all the labelling and categorical variables.

The `docs/` directory contains my [Quarto][quarto] source files that I write, both [Quarto][quarto] and [RMarkdown][rmd] will make a
bunch of other directories when they are rendering your pages which are some of those listed such as `_site` but you
don't need to worry about those.

The final key directory that I have is `r/` which holds the R scripts that I write.
:::

## Directory Structure - `data/`

<!-- :::: {.columns} -->

<!-- ::: {.column width="60%"} -->

```
❱ tree data
[4.0K Nov 22 14:57]  data
├── [4.0K Jan  9 13:32]  data/csv
│   ├── [ 20K Jan  9 13:31]  data/csv/sheffield_thyroid_nodule.csv
│   └── [393K Nov  2 11:08]  data/csv/Thy3000_DATA_LABELS_Raw.csv
└── [4.0K Feb 29 17:27]  data/r
    ├── [ 30K Mar 14 11:12]  data/r/clean.rds
    ├── [136K Feb 29 17:56]  data/r/elastic_net.RData
    ├── [136K Feb 29 17:56]  data/r/lasso.RData
    ├── [129K Feb 29 17:56]  data/r/rf.RData
    ├── [137K Feb 29 17:58]  data/r/svm.RData
    └── [ 14K Feb 29 17:56]  data/r/xgboost.RData

3 directories, 9 files
```

<!-- ::: -->
<!-- ::: {.column width="40%"} -->

<!-- Files -->

+ `data/csv/*.csv` - CSV files
+ `data/r/*.rds` - R files
+ `data/r/*.RData` - R files

<!-- ::: -->
<!-- :::: -->


::: {.notes}
The `data/csv` directory holds the raw data and there are two files the raw data `sheffield_thyroid_nodule.csv` and the
data dictionary `Thy3000_DATA_LABELS_Raw.csv`.

The `data/r` directory holds the cleaned data in `clean.rds` and the results of fitting various different types of
models to the data in the `.RData` files.
:::



## Directory Structure - `docs/`


```
❱ l docs | grep -v "~"
drwxr-xr-x neil neil 4.0 KB Thu Mar 14 17:36:04 2024 .
drwxr-xr-x neil neil 4.0 KB Thu Mar 28 16:35:45 2024 ..
.rw-r--r-- neil neil  10 B  Thu Feb 15 15:16:48 2024 .gitignore
.rw-r--r-- neil neil  12 KB Thu Jan 25 15:35:49 2024 .modelling.qmd
drwxr-xr-x neil neil 4.0 KB Thu Mar 14 17:15:43 2024 .quarto
.rw-r--r-- neil neil 483 B  Thu Mar 14 16:59:28 2024 _quarto.yml
drwxr-xr-x neil neil 4.0 KB Thu Mar 14 17:21:20 2024 _site
.rw-r--r-- neil neil 408 B  Thu Mar 14 16:57:10 2024 about.qmd
.rw-r--r-- neil neil 1.2 KB Thu Mar 14 16:57:10 2024 citations.qmd
lrwxrwxrwx neil neil   7 B  Fri Jan  5 14:45:10 2024 data ⇒ ../data
.rw-r--r-- neil neil  33 KB Thu Mar 14 16:57:10 2024 data.qmd
.rw-r--r-- neil neil 363 B  Thu Mar 14 16:57:10 2024 index.qmd
.rw-r--r-- neil neil 569 B  Thu Mar 14 16:57:10 2024 links.qmd
.rw-r--r-- neil neil 2.0 KB Thu Mar 14 16:57:10 2024 literature.qmd
.rw-r--r-- neil neil  32 KB Thu Mar 14 17:36:04 2024 modelling.qmd
drwxr-xr-x neil neil 4.0 KB Thu Feb 29 15:38:53 2024 modelling_cache
.rw-r--r-- neil neil 3.5 KB Thu Mar 14 17:31:46 2024 modelling_elastic_net.qmd
drwxr-xr-x neil neil 4.0 KB Thu Mar 14 17:21:18 2024 modelling_files
.rw-r--r-- neil neil 5.4 KB Thu Mar 14 17:31:46 2024 modelling_lasso.qmd
.rw-r--r-- neil neil 3.6 KB Thu Mar 14 17:31:46 2024 modelling_random_forest.qmd
.rw-r--r-- neil neil 7.0 KB Thu Mar 14 17:31:46 2024 modelling_tidymodel.qmd
lrwxrwxrwx neil neil   4 B  Fri Jan  5 14:44:09 2024 r ⇒ ../r
.rw-r--r-- neil neil  18 KB Thu Mar 14 16:57:10 2024 references.bib
.rw-r--r-- neil neil 2.7 KB Thu Mar 14 16:57:10 2024 reproducibility.qmd
.rw-r--r-- neil neil  17 B  Thu Mar 14 16:59:28 2024 styles.css
```


::: {.notes}
The `docs/` directory holds my [Quarto][quarto] files which all end in `.qmd`. I don't like having large files to
navigate so split my code and papers into a number of smaller files that do specific things and include these from the
master document which is `index.qmd`. There is a `references.bib` file which contains citations so we can include
citations easily. The `_quarto.yml`  files is a key file which defines how the Quarto project is setup and rendered but
I won't cover that in detail here.
:::


## Directory Structure - `r/`


```
❱ l r | grep -v "~"
drwxr-xr-x neil neil 4.0 KB Thu Mar 14 16:59:28 2024 .
drwxr-xr-x neil neil 4.0 KB Thu Mar 28 16:35:45 2024 ..
.rw-r--r-- neil neil  28 KB Thu Mar 14 16:56:18 2024 clean.R
.rw-r--r-- neil neil 684 B  Thu Mar 14 16:59:28 2024 master.R
.rw-r--r-- neil neil  15 KB Thu Feb 29 14:40:13 2024 modelling.R
.rw-r--r-- neil neil 2.6 KB Thu Mar 14 16:59:28 2024 simulate_data.R
.rw-r--r-- neil neil 233 B  Thu Jan 25 17:02:10 2024 summary.R
.rw-r--r-- neil neil 2.9 KB Thu Mar 14 16:57:10 2024 tidymodel.R
```


::: {.notes}
The `r/` directory holds my R scripts, the key one is `master.R` which holds the main commands that setup the R
environment, load libraries and set relative directories. The `clean.R` file does what it says on the tin, it cleans the
raw data and saves it in R format. There are then scripts for modelling and simulating data.

Now that we've covered how I organise my data we can start looking at some code, but before I move on are there any
questions about this aspect?
:::


## Scripts/Code - `master.R`

```{.r}
## Filename    : master.R
## Description : Master file that controls running of all subsequent scripts.
library(dplyr)
library(forcats)
library(Hmisc)
library(lubridate)
library(tidymodels)
library(tidyverse)
library(vip)

## Set directories based on current location
base_dir <- getwd()
data_dir <- paste(base_dir, "data", sep = "/")
csv_dir <- paste(data_dir, "csv", sep = "/")
r_dir <- paste(data_dir, "r", sep = "/")
r_scripts <- paste(base_dir, "r", sep = "/")

## Clean the data
source(paste(r_scripts, "clean.R", sep = "/"))


## Simulate data
source(paste(r_scripts, "simulate_data.R", sep = "/"))

## Run Statistical models
source(paste(r_scripts, "tidymodel.R", sep = "/"))
```

::: {.notes}
The `master.R` script doesn't do a great deal, it loads libraries and then as the comments show, and its important to
get into the habit of putting comments in scripts to help other people, including your future self, it then sets the
working directory. It does this in a reproducible manner by first asking the operating system what the current directory
is using the `getwd()` command and then appends various directory paths, such as `data` and saving it in the `data_dir`
variable, this in turn has `csv` appended to it and saved in the `csv_dir` which points to the nested directory where
our data files live.

You should get in the habit of using relative directories so that the code runs on different computers/systems, if you
had hard coded the path based on where you store the files on your computer the script won't run on any other computer
until someone changes that value, relative paths get around that limitation.

After setting directories a number of scripts are called using the `source()` command and within it I just paste the
name of the script onto the `r_scripts` variable which holds the path to the `r/` directory. The `clean.R` script is
first called, followed by the `simulate_data.R` and then the `tidymodels.R`.

:::

## Scripts/code - `r/clean.R`

```{.r}
## Filename : clean.R
## Description : Load and clean raw data, saving as a .RData file for subsequent analyses.

## Read the data and convert to tibble
df_raw <- read_csv(paste(csv_dir, "Thy3000_DATA_LABELS_Raw.csv", sep = "/"))
df_raw <- as_tibble(df_raw)
## Rename columns using dplyr::rename()
df_raw <- dplyr::rename(df_raw,
  record_id = "Record ID",
  data_access_group = "Data Access Group",
  study_id = "Study ID",
  date_referral = "1.1 Date of referral",
  clinic_recruiting = "1.2 Which clinic was the patient recruited from?",
  clinic_recruiting_other = "If Other",
  date_clinic = "1.3. The date the patient was seen in clinic",
  referral_source = "1.4 Referral source",
  referral_source_other = "If Other, please specify",
  two_week_wait_referral = "1.4.1 If GP, was it 2-week wait referral?",
  presentation = "1.5. Presentation",
  presentation_complete = "Complete?...12",
  age = "2.1. Age of the patient when seen in clinic",
  bmi = "2.2. Body Mass Index of patient",
  smoking_status = "2.3. Smoking status",
  ...
  )
```

::: {.notes}
Its important to script the work you do on the code, it provides evidence of the changes that have been made to clean
the code up and allows the work to be fully reproducible. Such an audit trail is useful as it allows mistakes to be
corrected. This and other R and Quarto Markdown files are version controlled using Git which I'll come onto shortly.

This script, `r/clean.R` does the hard work of cleaning/tidying the data.

It loads the data in and then renames a bunch of variables because spaces in names are a pain and all the numerical
prefixes are pointless.
:::

## Scripts/code - `r/clean.R` (_cont._)

```{.r}

## Convert dates to elapsed dates using lubridate
df <- df |>
  mutate(
    date_referral = lubridate::dmy(date_referral),
    date_clinic = lubridate::dmy(date_clinic),
    date_initial_management_decision = lubridate::dmy(date_initial_management_decision),
    date_treatment = lubridate::dmy(date_treatment),
    routine_review_date_last_seen = lubridate::dmy(routine_review_date_last_seen)
  )

## Convert variables that are meant to be numeric but aren't
df <- df |>
  dplyr::mutate(
    bmi = as.numeric(bmi),
  )
```

+ [Tidyverse][tv]
  + [`{dplyr}`][dplyr]
  + [`{lubridate}`][lubridate]
  + [R for DataScience (2e)][r4ds]

::: {.notes}
Another important step to working with data is to ensure the dates are correct and recognised by the software, the
strings people typically enter these as are not something that computers can do calculations on, instead software uses
elapsed times and takes as [epoch time](https://www.epochconverter.com/) which is the number seconds that have elapsed
from midnight on 1970-01-01 and so internally these are stored as numeric values but they can be represented as human
readable dates. But the advantage of having them stored as epoch time is that you can then perform calculations on them
and work out periods of time.

Often unless data capture has been done very carefully there will be errors in numerical variables such as including
characters which means the whole column for something that is meant to be numeric such as Body Mass Index (`bmi`) are
not the correct type, they're strings rather than actual numbers. We can use the `as.numeric()` function here with the
Tidyverse package `dplyr::mutate()` to replace these.

Its worth mentioning the Tidyverse which is an "_opinionated collection of R packages for data science_" that make
working with data in R considerably easier than the base functions. I would highly recommend the book.

Perhaps the biggest and best feature of the Tidyverse is the pipe, here I'm using the base pipe `|>` which allows
commands to be written in a Western-style of grammar, reading from left to right and top down rather than in base R
where you would nest functions within each other.

This vastly improves the readability of the code.
:::


## Scripts/code - `r/clean.R` (_cont._)

```{.r}

check_cols <- c(
  "previous_neck_irradiation",
  "incidental_imaging",
  "retrosternal",
  "palpable_lymphadenopathy",
  "nodule_rapid_growth",
  "thyroid_function_3months",
  "ultrasound",
  "elastography",
  "ct_neck",
  "mri_neck",
  "iodine_scan",
  "nodule_fna",
  "core_biopsy",
  "routine_review_ultrasound",
  "routine_review_fna",
  "routine_review_patient_signposting_information"
)
df <- df %>%
  dplyr::mutate(across(
    all_of(check_cols),
    ~ dplyr::recode(.x,
      "Yes" = 1,
      "No" = 0,
      "Not Known" = NA_real_
    )
  ))
```

+ [thyroid-cancer-prediction/r/clean.R](https://github.com/ns-rse/thyroid-cancer-prediction/blob/main/r/clean.R)

::: {.notes}
Another common task in cleaning data is to convert string variables such as `Yes` / `No` / `Not Known` to numerical
binary variables. Typically there will be lots of columns on which you want to do this and rather than write the same
code out lots there are a lot of neat ways of repeating commands across a range of variables.

Here we save all the variables we want to convert in the `check_cols` variable and then within the `dplyr::mutate()`
function we apply the same function `dplyr::recode()` `across()` `all_of()` this list, replacing `Yes` with `1`, `No`
with `0` and `Not Known` with R's internal `NA` value.

Currently it has 704 lines so I can't tal you through all of it but because this is all open if you want to look at the
code you can do and I've included the link on this slide.
:::


## Scripts/code - `r/clean.R` (_cont._)

```{.r}
## Finally save the data
saveRDS(df, file = paste(r_dir, "clean.rds", sep = "/"))
```

+ [thyroid-cancer-prediction/r/clean.R](https://github.com/ns-rse/thyroid-cancer-prediction/blob/main/r/clean.R)

::: {.notes}
Finally though we save the `df` data frame in the `r_dir` directory that was defined in the `master.R` script.

I can re-run this at any time if for example there was an error in my code, or more patients have been collected and the
dataset needs analysing again or conversely if someone withdraws from the study I could add lines to `r/clean.R` to
remove them from the dataset that is being analysed.
:::


## Version Control

+ Keep fine-grained records of changes to code.
+ Regular saves of work.
+ Backup and collaboration via [GitHub][gh] / [GitLab][gl]
+ Organised and consistent system, avoids...

::: {.notes}
I've mentioned that the files for this work are version controlled, hands up anyone who has heared of Version Control?

Keep your hands up if you've used version control?

Ok, thank you.

Version control has been around for a long time, you may be familiar with editions of books as one example or with
Wikipedia where the changes and edits that people make are tracked

:::

## Version Control

![](https://www.phdcomics.com/comics/archive/phd101212s.gif)


## Version Control

Two repositories on [GitHub][gh]

+ [ns-rse/thyroid-cancer-prediction](https://github.com/ns-rse/thyroid-cancer-prediction) - repository I started to show
  colleague how to use Git and R.
+ [mdp21oe/multicenter-thyroid](https://github.com/mdp21oe/multicentre_thyroid) - repository by Ovie to re-write his
  paper in Quarto Markdown.

<!--
## Formatting

Quarto slides are written in [Markdown](https://quarto.org/docs/authoring/markdown-basics.html).

| Markdown Syntax                              | Output                                |
|:---------------------------------------------|:--------------------------------------|
| `*italics*`, `**bold**`, `***bolditalics***` | *italics*, **bold**, **bold italics** |
| `superscript^1^`, `subscript~2~`             | superscript^1^, subscript~2~          |
| `~~strikethrough~~`                          | ~~strikethrough~~                     |
| ```code```                                   | `code`                                |
| `[Quarto](https://quarto.org)`               | [Quarto](https://quarto.org)          |

::: {.notes}
Speaker Notes can be added to each slide. **Formatting** can be used here too.
:::


## Embed Images

[Figures](https://quarto.org/docs/authoring/figures.html) can be embeded using a URL and resized.

```markdown
![Relaxing in the Julian Alps](https://live.staticflickr.com/65535/53144704609_c5e6fa8c77_k.jpg){width=700}
```

![Relaxing in the Julian Alps](https://live.staticflickr.com/65535/53144704609_c5e6fa8c77_k.jpg){width=700}

::: {.notes}
:::

## Embed Images (*cont.*)

You can also include images locally.^[Make sure to `git add` them otherwise they won't publish] and make them hyperlinks.

```markdown
![[OSC Sheffield](https://osc-international.com/osc-sheffield)](img/OSC_Sheffield.png){width=300}
```

![[OSC Sheffield](https://osc-international.com/osc-sheffield)](img/OSC_Sheffield.png){width=300}

::: {.notes}
:::

-->

## Quarto & Data Science Learning Community

:::: {.columns}

::: {.column width="50%"}
The official documentation is _really_ good.

+ [Get Started](https://quarto.org/docs/get-started/)
+ [Markdown Basics](https://quarto.org/docs/authoring/markdown-basics.html)
+ [Guide](https://quarto.org/docs/guide/) with many sections on [Presentations](https://quarto.org/docs/presentations/)
  [Website](https://quarto.org/docs/websites/), [Books](https://quarto.org/docs/books/) and more.
:::
::: {.column width=50%}



{{< qrcode <https://dslc.io/> qr3 width=400 height=400 >}}
[Data Science Learning Community (dslc.io)](https://dslc.io/)
:::
::::


::: {.notes}
I think most of the course material you're learning is focused on RMarkdown but everything you learn can be applied to
Quarto which is really the next iteration or generation of RMarkdown. If you're keen to give it a try then the
documentation is really good.

Perhaps also of interest when you have questions about things which aren't working as you'd expect or how to do
something is the execellent [Data Science Learning Community (dslc.io)](https://dslc.io/) (need R4DS community) which is
a great place to join book clubs to work through things like [R for Data Science (2e)](https://r4ds.hadley.nz/) and over
50 books clubs or ask questions in their Slack channel.
:::
<!-- ## Extensions -->

<!-- :::: {.columns} -->

<!-- ::: {.column width="70%"} -->
<!-- [Extensions](https://quarto.org/docs/extensions/) for themes and other functionality -->
<!-- (press `c` whilst viewing these slides :wink: ). -->

<!-- + [Shortcode/Filter](https://quarto.org/docs/extensions/listing-filters.html) -->
<!-- + [Journal Articles](https://quarto.org/docs/extensions/listing-journals.html) -->
<!-- + [Custom Formats](https://quarto.org/docs/extensions/listing-formats.html) -->
<!-- + [**RevealJS**](https://quarto.org/docs/extensions/listing-revealjs.html) -->
<!-- + [awesome-quarto](https://github.com/mcanouil/awesome-quarto) - Quarto tools & examples -->

<!-- ::: -->
<!-- ::: {.column width=30%} -->
<!-- This Template... -->
<!-- {{< qrcode <https://ns-rse.github.io/quarto-revealjs-template> qr2 width=400 height=400 >}} -->
<!-- ::: -->
<!-- :::: -->

## SheffieldR User Group

:::: {.columns}

::: {.column width="50%"}
[![sheffieldr.github.io](https://sheffieldr.github.io/schedule_slides/img/SheffieldR_hex.png)](https://sheffieldr.github.io)
:::
::: {.column width=50%}

{{< qrcode <https://sheffieldr.github.io/> qr2 width=400 height=400 >}}
[sheffieldr.github.io](https://sheffieldr.github.io)

:::
::::

::: {.notes}
If you're interested in learning more about the many wonderful things you can do with R then you might be interested in
the SheffieldR User Group that meets roughly every month with presentations and talks on different packages and how to
do different things with R.

Its meant to be diverse, friendly and welcoming to users of all abilities.
:::

[dplyr]: https://dplyr.tidyverse.org/
[gh]: https://github.com
[git]: https://git-scm.com
[gl]: https://gitlab.com
[lubridate]: https://lubridate.tidyverse.org/
[quarto]: https://quarto.org
[r4ds]: https://r4ds.hadley.nz/
[rmd]: https://bookdown.org/yihui/rmarkdown/
[tv]: https://www.tidyverse.org/
